{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as nprand\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math as math\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2, f_classif \n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loading and preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The original data = df\n",
    "df = pd.read_csv('data.csv',\n",
    "    header=None,\n",
    "    index_col=False).T"
   ]
  },
  {
   "source": [
    "Select labels, light and feature"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lesions=df.iloc[:,1].apply(pd.to_numeric)\n",
    "light=df.iloc[:,2].apply(pd.to_numeric)\n",
    "features=df.iloc[:,3:-1].apply(pd.to_numeric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = lesions.values.reshape(len(lesions), 1)\n",
    "lesions_encoded = onehot_encoder.fit_transform(integer_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, lesions_encoded, test_size=0.2,stratify=lesions_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3    80\n",
       "1    42\n",
       "2    30\n",
       "Name: 1, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 136
    }
   ],
   "source": [
    "lesions.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import keras.layers as layers\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "def normalize_with_moments(x, axes=[0, 1], epsilon=1e-8):\n",
    "    mean, variance = tf.nn.moments(x, axes=axes)\n",
    "    x_normed = (x - mean) / tf.sqrt(variance + epsilon) # epsilon to avoid dividing by zero\n",
    "    return x_normed\n",
    "\n",
    "class calculateInteraction(layers.Layer):\n",
    "    def __init__(self,featureSize):\n",
    "        super(calculateInteraction, self).__init__()\n",
    "        mask = tf.random_normal_initializer()\n",
    "        self.mask = tf.Variable(\n",
    "            initial_value=mask(shape=(1, featureSize), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "        randomIn = tf.random_normal_initializer()\n",
    "        self.w = tf.Variable(\n",
    "            initial_value=randomIn(shape=(featureSize, 1), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "    \n",
    "    def call(self, x):\n",
    "\n",
    "        Mx=tf.multiply(self.mask,x)\n",
    "        query_value_attention_seq = layers.Attention()([x, Mx])\n",
    "        query_value_attention_seq=tf.expand_dims(query_value_attention_seq,2)\n",
    "        query_value_attention_seq=normalize_with_moments(query_value_attention_seq)\n",
    "        query_value_attention = layers.GlobalAveragePooling1D()(\n",
    "            query_value_attention_seq)\n",
    "        Mx=tf.expand_dims(Mx,2)\n",
    "        # query_value_attention_seq=tf.multiply(query_value_attention_seq,self.w)\n",
    "        concatenated = layers.Concatenate()(\n",
    "            [Mx, query_value_attention_seq])\n",
    "     \n",
    "        print(concatenated.shape)\n",
    "        return concatenated\n",
    "\n",
    "def getModel(inputSize):\n",
    "    \n",
    "    inputs = layers.Input(shape=inputSize)\n",
    "    x=layers.Dense(24,activation='relu')(inputs)\n",
    "    concatenated=calculateInteraction(24)(x)\n",
    "    concatenated=layers.Flatten()(concatenated)\n",
    "    output=layers.Dense(3,activation='softmax')(concatenated)\n",
    "    \n",
    "    METRICS = [\n",
    "      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      tf.keras.metrics.Precision(name='precision'),\n",
    "      tf.keras.metrics.Recall(name='recall'),\n",
    "      tf.keras.metrics.AUC(name='auc'),\n",
    "     ]   \n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    model.compile(tf.keras.optimizers.Adam(lr=0.001),loss='categorical_crossentropy',metrics=METRICS)\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(None, 24, 2)\nModel: \"model_26\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_32 (InputLayer)        [(None, 697)]             0         \n_________________________________________________________________\ndense_51 (Dense)             (None, 24)                16752     \n_________________________________________________________________\ncalculate_interaction_31 (ca (None, 24, 2)             48        \n_________________________________________________________________\nflatten_28 (Flatten)         (None, 48)                0         \n_________________________________________________________________\ndense_52 (Dense)             (None, 3)                 147       \n=================================================================\nTotal params: 16,947\nTrainable params: 16,947\nNon-trainable params: 0\n_________________________________________________________________\nModel: \"model_26\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_32 (InputLayer)        [(None, 697)]             0         \n_________________________________________________________________\ndense_51 (Dense)             (None, 24)                16752     \n_________________________________________________________________\ncalculate_interaction_31 (ca (None, 24, 2)             48        \n_________________________________________________________________\nflatten_28 (Flatten)         (None, 48)                0         \n_________________________________________________________________\ndense_52 (Dense)             (None, 3)                 147       \n=================================================================\nTotal params: 16,947\nTrainable params: 16,947\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "total=lesions_encoded.shape[0]\n",
    "c1=len(np.where(lesions_encoded[:,0]==1)[0])\n",
    "c2=len(np.where(lesions_encoded[:,1]==1)[0])\n",
    "c3=len(np.where(lesions_encoded[:,2]==1)[0])\n",
    "\n",
    "model=getModel(X_train.shape[1])\n",
    "\n",
    "batch_size=32\n",
    "weight_for_0 = (batch_size / c1)*(total)/3.0 \n",
    "weight_for_1 = (batch_size / c2)*(total)/3.0\n",
    "weight_for_2 = (batch_size/ c3)*(total)/3.0\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "===] - 0s 13ms/step - loss: 37.3799 - accuracy: 0.8142 - precision: 0.7225 - recall: 0.7188 - auc: 0.8932 - val_loss: 1.5808 - val_accuracy: 0.7867 - val_precision: 0.6957 - val_recall: 0.6400 - val_auc: 0.8020\n",
      "Epoch 424/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 26.2847 - accuracy: 0.7643 - precision: 0.6566 - recall: 0.6211 - auc: 0.8134 - val_loss: 2.5395 - val_accuracy: 0.6533 - val_precision: 0.4783 - val_recall: 0.4400 - val_auc: 0.6924\n",
      "Epoch 425/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 36.1831 - accuracy: 0.6658 - precision: 0.4998 - recall: 0.4909 - auc: 0.7518 - val_loss: 1.5827 - val_accuracy: 0.7867 - val_precision: 0.6800 - val_recall: 0.6800 - val_auc: 0.7848\n",
      "Epoch 426/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 26.0670 - accuracy: 0.8056 - precision: 0.7083 - recall: 0.7083 - auc: 0.9042 - val_loss: 1.5346 - val_accuracy: 0.7867 - val_precision: 0.6800 - val_recall: 0.6800 - val_auc: 0.7780\n",
      "Epoch 427/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 26.9004 - accuracy: 0.7665 - precision: 0.6502 - recall: 0.6471 - auc: 0.8411 - val_loss: 2.1109 - val_accuracy: 0.6400 - val_precision: 0.4583 - val_recall: 0.4400 - val_auc: 0.7380\n",
      "Epoch 428/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 34.0758 - accuracy: 0.7049 - precision: 0.5589 - recall: 0.5456 - auc: 0.7737 - val_loss: 1.4046 - val_accuracy: 0.7067 - val_precision: 0.5600 - val_recall: 0.5600 - val_auc: 0.8124\n",
      "Epoch 429/500\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 31.5894 - accuracy: 0.8260 - precision: 0.7513 - recall: 0.7135 - auc: 0.8892 - val_loss: 1.2399 - val_accuracy: 0.8267 - val_precision: 0.7500 - val_recall: 0.7200 - val_auc: 0.8280\n",
      "Epoch 430/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 13.7145 - accuracy: 0.8598 - precision: 0.7922 - recall: 0.7852 - auc: 0.9350 - val_loss: 1.6374 - val_accuracy: 0.7867 - val_precision: 0.6957 - val_recall: 0.6400 - val_auc: 0.7860\n",
      "Epoch 431/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 19.4390 - accuracy: 0.7652 - precision: 0.6521 - recall: 0.6341 - auc: 0.8418 - val_loss: 1.2663 - val_accuracy: 0.8267 - val_precision: 0.7500 - val_recall: 0.7200 - val_auc: 0.8236\n",
      "Epoch 432/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 15.4162 - accuracy: 0.8355 - precision: 0.7578 - recall: 0.7448 - auc: 0.9206 - val_loss: 1.4175 - val_accuracy: 0.7600 - val_precision: 0.6400 - val_recall: 0.6400 - val_auc: 0.7924\n",
      "Epoch 433/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 13.8375 - accuracy: 0.8129 - precision: 0.7248 - recall: 0.7070 - auc: 0.9080 - val_loss: 1.3854 - val_accuracy: 0.8400 - val_precision: 0.7600 - val_recall: 0.7600 - val_auc: 0.8152\n",
      "Epoch 434/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 14.4579 - accuracy: 0.8607 - precision: 0.7936 - recall: 0.7865 - auc: 0.9170 - val_loss: 1.4914 - val_accuracy: 0.7867 - val_precision: 0.6800 - val_recall: 0.6800 - val_auc: 0.7912\n",
      "Epoch 435/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 14.0747 - accuracy: 0.8446 - precision: 0.7669 - recall: 0.7669 - auc: 0.9295 - val_loss: 1.3301 - val_accuracy: 0.8533 - val_precision: 0.7917 - val_recall: 0.7600 - val_auc: 0.8376\n",
      "Epoch 436/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 14.5962 - accuracy: 0.8507 - precision: 0.7937 - recall: 0.7461 - auc: 0.9023 - val_loss: 1.3744 - val_accuracy: 0.8400 - val_precision: 0.7600 - val_recall: 0.7600 - val_auc: 0.8216\n",
      "Epoch 437/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 14.5957 - accuracy: 0.8099 - precision: 0.7215 - recall: 0.6979 - auc: 0.9094 - val_loss: 1.2955 - val_accuracy: 0.8533 - val_precision: 0.7917 - val_recall: 0.7600 - val_auc: 0.8392\n",
      "Epoch 438/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 12.7798 - accuracy: 0.8689 - precision: 0.8104 - recall: 0.7917 - auc: 0.9354 - val_loss: 1.3211 - val_accuracy: 0.8133 - val_precision: 0.7200 - val_recall: 0.7200 - val_auc: 0.8360\n",
      "Epoch 439/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 12.9042 - accuracy: 0.8455 - precision: 0.7682 - recall: 0.7682 - auc: 0.9317 - val_loss: 1.3072 - val_accuracy: 0.8533 - val_precision: 0.7917 - val_recall: 0.7600 - val_auc: 0.8364\n",
      "Epoch 440/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 13.1414 - accuracy: 0.8520 - precision: 0.7805 - recall: 0.7734 - auc: 0.9326 - val_loss: 1.3237 - val_accuracy: 0.8533 - val_precision: 0.7917 - val_recall: 0.7600 - val_auc: 0.8452\n",
      "Epoch 441/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 11.8502 - accuracy: 0.8641 - precision: 0.8087 - recall: 0.7760 - auc: 0.9331 - val_loss: 1.2988 - val_accuracy: 0.8667 - val_precision: 0.8261 - val_recall: 0.7600 - val_auc: 0.8272\n",
      "Epoch 442/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 14.7208 - accuracy: 0.8494 - precision: 0.7767 - recall: 0.7695 - auc: 0.9264 - val_loss: 1.3201 - val_accuracy: 0.8533 - val_precision: 0.7917 - val_recall: 0.7600 - val_auc: 0.8372\n",
      "Epoch 443/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 14.1655 - accuracy: 0.8299 - precision: 0.7461 - recall: 0.7422 - auc: 0.9103 - val_loss: 1.2995 - val_accuracy: 0.8400 - val_precision: 0.7600 - val_recall: 0.7600 - val_auc: 0.8368\n",
      "Epoch 444/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 11.7935 - accuracy: 0.8780 - precision: 0.8198 - recall: 0.8125 - auc: 0.9549 - val_loss: 1.3290 - val_accuracy: 0.8133 - val_precision: 0.7200 - val_recall: 0.7200 - val_auc: 0.8404\n",
      "Epoch 445/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 13.6725 - accuracy: 0.8472 - precision: 0.7815 - recall: 0.7513 - auc: 0.9216 - val_loss: 1.3589 - val_accuracy: 0.8400 - val_precision: 0.7600 - val_recall: 0.7600 - val_auc: 0.8388\n",
      "Epoch 446/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 13.7264 - accuracy: 0.8290 - precision: 0.7530 - recall: 0.7240 - auc: 0.9167 - val_loss: 1.3349 - val_accuracy: 0.8267 - val_precision: 0.7500 - val_recall: 0.7200 - val_auc: 0.8344\n",
      "Epoch 447/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 12.4493 - accuracy: 0.8711 - precision: 0.8095 - recall: 0.8021 - auc: 0.9401 - val_loss: 1.3517 - val_accuracy: 0.8267 - val_precision: 0.7500 - val_recall: 0.7200 - val_auc: 0.8300\n",
      "Epoch 448/500\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 13.6220 - accuracy: 0.8542 - precision: 0.7888 - recall: 0.7682 - auc: 0.9245 - val_loss: 1.3343 - val_accuracy: 0.8400 - val_precision: 0.7600 - val_recall: 0.7600 - val_auc: 0.8224\n",
      "Epoch 449/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 10.6396 - accuracy: 0.8837 - precision: 0.8305 - recall: 0.8177 - auc: 0.9570 - val_loss: 1.3258 - val_accuracy: 0.8267 - val_precision: 0.7500 - val_recall: 0.7200 - val_auc: 0.8264\n",
      "Epoch 450/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 11.2280 - accuracy: 0.8819 - precision: 0.8246 - recall: 0.8203 - auc: 0.9433 - val_loss: 1.3499 - val_accuracy: 0.8400 - val_precision: 0.7600 - val_recall: 0.7600 - val_auc: 0.8364\n",
      "Epoch 451/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 12.1905 - accuracy: 0.8559 - precision: 0.7954 - recall: 0.7643 - auc: 0.9313 - val_loss: 1.3474 - val_accuracy: 0.8400 - val_precision: 0.7600 - val_recall: 0.7600 - val_auc: 0.8372\n",
      "Epoch 452/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 12.8000 - accuracy: 0.8720 - precision: 0.8192 - recall: 0.7904 - auc: 0.9306 - val_loss: 1.3396 - val_accuracy: 0.8533 - val_precision: 0.7917 - val_recall: 0.7600 - val_auc: 0.8236\n",
      "Epoch 453/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 14.0519 - accuracy: 0.8329 - precision: 0.7548 - recall: 0.7383 - auc: 0.9268 - val_loss: 1.3655 - val_accuracy: 0.8267 - val_precision: 0.7500 - val_recall: 0.7200 - val_auc: 0.8380\n",
      "Epoch 454/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 11.2336 - accuracy: 0.8737 - precision: 0.8164 - recall: 0.8008 - auc: 0.9414 - val_loss: 1.3613 - val_accuracy: 0.8400 - val_precision: 0.7826 - val_recall: 0.7200 - val_auc: 0.8332\n",
      "Epoch 455/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 13.0898 - accuracy: 0.8598 - precision: 0.7955 - recall: 0.7799 - auc: 0.9283 - val_loss: 1.3570 - val_accuracy: 0.8400 - val_precision: 0.7826 - val_recall: 0.7200 - val_auc: 0.8320\n",
      "Epoch 456/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 12.5511 - accuracy: 0.8741 - precision: 0.8129 - recall: 0.8086 - auc: 0.9373 - val_loss: 1.3492 - val_accuracy: 0.8667 - val_precision: 0.8261 - val_recall: 0.7600 - val_auc: 0.8224\n",
      "Epoch 457/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 12.3682 - accuracy: 0.8850 - precision: 0.8322 - recall: 0.8203 - auc: 0.9364 - val_loss: 1.3601 - val_accuracy: 0.8533 - val_precision: 0.7917 - val_recall: 0.7600 - val_auc: 0.8364\n",
      "Epoch 458/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 12.2572 - accuracy: 0.8689 - precision: 0.8176 - recall: 0.7812 - auc: 0.9325 - val_loss: 1.3427 - val_accuracy: 0.8400 - val_precision: 0.7826 - val_recall: 0.7200 - val_auc: 0.8324\n",
      "Epoch 459/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 11.9018 - accuracy: 0.8767 - precision: 0.8236 - recall: 0.8021 - auc: 0.9398 - val_loss: 1.3472 - val_accuracy: 0.8133 - val_precision: 0.7391 - val_recall: 0.6800 - val_auc: 0.8320\n",
      "Epoch 460/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 12.1337 - accuracy: 0.8785 - precision: 0.8298 - recall: 0.7995 - auc: 0.9392 - val_loss: 1.3481 - val_accuracy: 0.8133 - val_precision: 0.7391 - val_recall: 0.6800 - val_auc: 0.8312\n",
      "Epoch 461/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 11.5933 - accuracy: 0.8650 - precision: 0.8111 - recall: 0.7760 - auc: 0.9541 - val_loss: 1.3600 - val_accuracy: 0.8400 - val_precision: 0.7826 - val_recall: 0.7200 - val_auc: 0.8328\n",
      "Epoch 462/500\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 13.8411 - accuracy: 0.8524 - precision: 0.7817 - recall: 0.7734 - auc: 0.9251 - val_loss: 1.3624 - val_accuracy: 0.8533 - val_precision: 0.7917 - val_recall: 0.7600 - val_auc: 0.8304\n",
      "Epoch 463/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 12.0529 - accuracy: 0.8815 - precision: 0.8269 - recall: 0.8151 - auc: 0.9452 - val_loss: 1.3614 - val_accuracy: 0.8267 - val_precision: 0.7500 - val_recall: 0.7200 - val_auc: 0.8292\n",
      "Epoch 464/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 11.4978 - accuracy: 0.8655 - precision: 0.8013 - recall: 0.7930 - auc: 0.9422 - val_loss: 1.3669 - val_accuracy: 0.8133 - val_precision: 0.7391 - val_recall: 0.6800 - val_auc: 0.8308\n",
      "Epoch 465/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 11.7274 - accuracy: 0.8741 - precision: 0.8144 - recall: 0.8060 - auc: 0.9422 - val_loss: 1.3593 - val_accuracy: 0.8400 - val_precision: 0.7826 - val_recall: 0.7200 - val_auc: 0.8180\n",
      "Epoch 466/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 13.2791 - accuracy: 0.8819 - precision: 0.8333 - recall: 0.8073 - auc: 0.9368 - val_loss: 1.3708 - val_accuracy: 0.8533 - val_precision: 0.7917 - val_recall: 0.7600 - val_auc: 0.8336\n",
      "Epoch 467/500\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 12.5988 - accuracy: 0.8646 - precision: 0.8089 - recall: 0.7773 - auc: 0.9263 - val_loss: 1.3609 - val_accuracy: 0.8133 - val_precision: 0.7391 - val_recall: 0.6800 - val_auc: 0.8176\n",
      "Epoch 468/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 12.6898 - accuracy: 0.8472 - precision: 0.7796 - recall: 0.7552 - auc: 0.9381 - val_loss: 1.3722 - val_accuracy: 0.8000 - val_precision: 0.7083 - val_recall: 0.6800 - val_auc: 0.8296\n",
      "Epoch 469/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 11.7676 - accuracy: 0.8785 - precision: 0.8309 - recall: 0.7982 - auc: 0.9391 - val_loss: 1.3803 - val_accuracy: 0.8267 - val_precision: 0.7500 - val_recall: 0.7200 - val_auc: 0.8308\n",
      "Epoch 470/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 12.1351 - accuracy: 0.8542 - precision: 0.7888 - recall: 0.7682 - auc: 0.9392 - val_loss: 1.3724 - val_accuracy: 0.8400 - val_precision: 0.7600 - val_recall: 0.7600 - val_auc: 0.8212\n",
      "Epoch 471/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 11.0851 - accuracy: 0.8772 - precision: 0.8205 - recall: 0.8086 - auc: 0.9518 - val_loss: 1.3918 - val_accuracy: 0.8267 - val_precision: 0.7500 - val_recall: 0.7200 - val_auc: 0.8288\n",
      "Epoch 472/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 11.9994 - accuracy: 0.8585 - precision: 0.7930 - recall: 0.7773 - auc: 0.9317 - val_loss: 1.3732 - val_accuracy: 0.8000 - val_precision: 0.7083 - val_recall: 0.6800 - val_auc: 0.8292\n",
      "Epoch 473/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 11.9986 - accuracy: 0.8819 - precision: 0.8287 - recall: 0.8138 - auc: 0.9493 - val_loss: 1.3789 - val_accuracy: 0.8267 - val_precision: 0.7500 - val_recall: 0.7200 - val_auc: 0.8180\n",
      "Epoch 474/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 12.1224 - accuracy: 0.8850 - precision: 0.8411 - recall: 0.8073 - auc: 0.9436 - val_loss: 1.4192 - val_accuracy: 0.8400 - val_precision: 0.7600 - val_recall: 0.7600 - val_auc: 0.8300\n",
      "Epoch 475/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 12.6699 - accuracy: 0.8511 - precision: 0.7845 - recall: 0.7630 - auc: 0.9198 - val_loss: 1.3974 - val_accuracy: 0.8000 - val_precision: 0.7083 - val_recall: 0.6800 - val_auc: 0.8264\n",
      "Epoch 476/500\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 13.3181 - accuracy: 0.8594 - precision: 0.7984 - recall: 0.7734 - auc: 0.9340 - val_loss: 1.4004 - val_accuracy: 0.8000 - val_precision: 0.7083 - val_recall: 0.6800 - val_auc: 0.8124\n",
      "Epoch 477/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 10.7971 - accuracy: 0.9006 - precision: 0.8559 - recall: 0.8438 - auc: 0.9546 - val_loss: 1.4244 - val_accuracy: 0.8267 - val_precision: 0.7500 - val_recall: 0.7200 - val_auc: 0.8296\n",
      "Epoch 478/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 12.0121 - accuracy: 0.8572 - precision: 0.7916 - recall: 0.7760 - auc: 0.9290 - val_loss: 1.3914 - val_accuracy: 0.8267 - val_precision: 0.7500 - val_recall: 0.7200 - val_auc: 0.8160\n",
      "Epoch 479/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 13.1356 - accuracy: 0.8724 - precision: 0.8168 - recall: 0.7956 - auc: 0.9437 - val_loss: 1.3857 - val_accuracy: 0.8267 - val_precision: 0.7500 - val_recall: 0.7200 - val_auc: 0.8180\n",
      "Epoch 480/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 12.3883 - accuracy: 0.8524 - precision: 0.7876 - recall: 0.7630 - auc: 0.9369 - val_loss: 1.4680 - val_accuracy: 0.8400 - val_precision: 0.7826 - val_recall: 0.7200 - val_auc: 0.8316\n",
      "Epoch 481/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 11.7630 - accuracy: 0.8568 - precision: 0.7974 - recall: 0.7643 - auc: 0.9330 - val_loss: 1.4167 - val_accuracy: 0.8000 - val_precision: 0.7083 - val_recall: 0.6800 - val_auc: 0.8100\n",
      "Epoch 482/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 13.9059 - accuracy: 0.8798 - precision: 0.8270 - recall: 0.8086 - auc: 0.9490 - val_loss: 1.4127 - val_accuracy: 0.8267 - val_precision: 0.7500 - val_recall: 0.7200 - val_auc: 0.8292\n",
      "Epoch 483/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 15.7599 - accuracy: 0.8455 - precision: 0.7750 - recall: 0.7539 - auc: 0.8903 - val_loss: 1.7171 - val_accuracy: 0.7733 - val_precision: 0.6667 - val_recall: 0.6400 - val_auc: 0.7948\n",
      "Epoch 484/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 20.4556 - accuracy: 0.7808 - precision: 0.6781 - recall: 0.6497 - auc: 0.8410 - val_loss: 1.4576 - val_accuracy: 0.8000 - val_precision: 0.7083 - val_recall: 0.6800 - val_auc: 0.8096\n",
      "Epoch 485/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 13.9171 - accuracy: 0.8555 - precision: 0.7883 - recall: 0.7747 - auc: 0.9483 - val_loss: 1.4189 - val_accuracy: 0.8267 - val_precision: 0.7500 - val_recall: 0.7200 - val_auc: 0.8292\n",
      "Epoch 486/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 14.6527 - accuracy: 0.8147 - precision: 0.7301 - recall: 0.7044 - auc: 0.8974 - val_loss: 1.4675 - val_accuracy: 0.8400 - val_precision: 0.7826 - val_recall: 0.7200 - val_auc: 0.8300\n",
      "Epoch 487/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 12.9137 - accuracy: 0.8247 - precision: 0.7419 - recall: 0.7279 - auc: 0.9180 - val_loss: 1.5058 - val_accuracy: 0.8000 - val_precision: 0.7083 - val_recall: 0.6800 - val_auc: 0.7996\n",
      "Epoch 488/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 16.6706 - accuracy: 0.8746 - precision: 0.8146 - recall: 0.8073 - auc: 0.9422 - val_loss: 1.4428 - val_accuracy: 0.8400 - val_precision: 0.7826 - val_recall: 0.7200 - val_auc: 0.8336\n",
      "Epoch 489/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 13.5500 - accuracy: 0.8407 - precision: 0.7672 - recall: 0.7500 - auc: 0.9080 - val_loss: 1.5006 - val_accuracy: 0.8267 - val_precision: 0.7500 - val_recall: 0.7200 - val_auc: 0.8260\n",
      "Epoch 490/500\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 13.9795 - accuracy: 0.8407 - precision: 0.7707 - recall: 0.7435 - auc: 0.9189 - val_loss: 1.4487 - val_accuracy: 0.8000 - val_precision: 0.7083 - val_recall: 0.6800 - val_auc: 0.8060\n",
      "Epoch 491/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 13.2712 - accuracy: 0.8533 - precision: 0.7890 - recall: 0.7643 - auc: 0.9386 - val_loss: 1.5346 - val_accuracy: 0.8133 - val_precision: 0.7391 - val_recall: 0.6800 - val_auc: 0.8224\n",
      "Epoch 492/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 12.5770 - accuracy: 0.8290 - precision: 0.7513 - recall: 0.7279 - auc: 0.9201 - val_loss: 1.4155 - val_accuracy: 0.8267 - val_precision: 0.7500 - val_recall: 0.7200 - val_auc: 0.8232\n",
      "Epoch 493/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 12.5456 - accuracy: 0.8607 - precision: 0.7937 - recall: 0.7865 - auc: 0.9471 - val_loss: 1.4080 - val_accuracy: 0.8133 - val_precision: 0.7391 - val_recall: 0.6800 - val_auc: 0.8268\n",
      "Epoch 494/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 12.6775 - accuracy: 0.8477 - precision: 0.7770 - recall: 0.7617 - auc: 0.9250 - val_loss: 1.5173 - val_accuracy: 0.8133 - val_precision: 0.7200 - val_recall: 0.7200 - val_auc: 0.8252\n",
      "Epoch 495/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 12.7340 - accuracy: 0.8459 - precision: 0.7786 - recall: 0.7513 - auc: 0.9182 - val_loss: 1.4316 - val_accuracy: 0.8000 - val_precision: 0.7083 - val_recall: 0.6800 - val_auc: 0.8128\n",
      "Epoch 496/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 14.9779 - accuracy: 0.8351 - precision: 0.7526 - recall: 0.7526 - auc: 0.9319 - val_loss: 1.4266 - val_accuracy: 0.8400 - val_precision: 0.7826 - val_recall: 0.7200 - val_auc: 0.8332\n",
      "Epoch 497/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 13.1489 - accuracy: 0.8537 - precision: 0.7908 - recall: 0.7630 - auc: 0.9183 - val_loss: 1.4685 - val_accuracy: 0.8267 - val_precision: 0.7500 - val_recall: 0.7200 - val_auc: 0.8332\n",
      "Epoch 498/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 11.7859 - accuracy: 0.8537 - precision: 0.7908 - recall: 0.7630 - auc: 0.9339 - val_loss: 1.4729 - val_accuracy: 0.8000 - val_precision: 0.7083 - val_recall: 0.6800 - val_auc: 0.8036\n",
      "Epoch 499/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 13.7264 - accuracy: 0.8741 - precision: 0.8196 - recall: 0.7982 - auc: 0.9516 - val_loss: 1.4678 - val_accuracy: 0.8133 - val_precision: 0.7200 - val_recall: 0.7200 - val_auc: 0.8280\n",
      "Epoch 500/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 14.1968 - accuracy: 0.8464 - precision: 0.7815 - recall: 0.7500 - auc: 0.9041 - val_loss: 1.5297 - val_accuracy: 0.8267 - val_precision: 0.7500 - val_recall: 0.7200 - val_auc: 0.8268\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1,2:weight_for_2}\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_recall', mode='max', verbose=1,baseline=0.75,patience=500)\n",
    "history=model.fit(X_train,y_train,epochs=500,batch_size=batch_size,validation_split=0.2,class_weight=class_weight,callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(None, 24, 2)\n              precision    recall  f1-score   support\n\n           1       0.57      0.89      0.70         9\n           2       0.38      0.50      0.43         6\n           3       0.78      0.44      0.56        16\n\n    accuracy                           0.58        31\n   macro avg       0.57      0.61      0.56        31\nweighted avg       0.64      0.58      0.57        31\n\n              precision    recall  f1-score   support\n\n           1       0.78      0.94      0.85        33\n           2       0.53      0.83      0.65        24\n           3       0.88      0.59      0.71        64\n\n    accuracy                           0.74       121\n   macro avg       0.73      0.79      0.73       121\nweighted avg       0.78      0.74      0.74       121\n\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "labelNames=['hyperplasic', 'serrated', 'adenoma']\n",
    "y_output=model.predict(X_test)\n",
    "y_output_transformed= onehot_encoder.inverse_transform(y_output)\n",
    "y_test_transformed= onehot_encoder.inverse_transform(y_test)\n",
    "print(classification_report(y_test_transformed, y_output_transformed, labels=[1,2,3]))\n",
    "\n",
    "y_output_train=model.predict(X_train)\n",
    "y_output_transformed_train= onehot_encoder.inverse_transform(y_output_train)\n",
    "y_train_transformed= onehot_encoder.inverse_transform(y_train)\n",
    "print(classification_report(y_train_transformed, y_output_transformed_train, labels=[1,2,3]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}